# Random effects logistic regression model analyzing how well
# psi predicts recognition memory, where psi is a computational model.
# Intercepts vary randomly by subject, item and text
# Effect of psi varies randomly by subject, text
# Effect of whether item is actually present or not in the to-be-rembered text varies randomly by text only
# Interaction effect of presence and model, but does not vary randomly

model {

    for (i in 1:N) {

        y[i] ~ dbern(p[i])
        p[i] <- ilogit(alpha.0 + 	
			(b + b.text[text[i]] + b.subject[subject[i]]) * psi[i] +
			(beta +  beta.text[text[i]]) * present[i] +
			gamma * present[i] * psi[i] +
			alpha.text[text[i]] + 
			alpha.subject[subject[i]] +
			alpha.item[item[i]])
    }

    for (k in 1:K){
        alpha.text[k] ~ dnorm(0.0, alpha.text.tau)
        b.text[k] ~ dnorm(0.0, b.text.tau)
        beta.text[k] ~ dnorm(0.0, beta.text.tau)
    }

    for (j in 1:J){
        alpha.subject[j] ~ dnorm(0.0, alpha.subject.tau)
        b.subject[j] ~ dnorm(0.0, b.subject.tau)
    }

    for (l in 1:L){
        alpha.item[l] ~ dnorm(0.0, alpha.item.tau)
    }

    alpha.text.sigma ~ dnorm(0.0, 1/100.0)T(0,)
    alpha.subject.sigma ~ dnorm(0.0, 1/100.0)T(0,)
    alpha.item.sigma ~ dnorm(0.0, 1/100.0)T(0,)

    alpha.text.tau <- 1/(alpha.text.sigma^2)
    alpha.subject.tau <- 1/(alpha.subject.sigma^2)
    alpha.item.tau <- 1/(alpha.item.sigma^2)

    b.text.sigma ~ dnorm(0.0, 1/100.0)T(0,)
    b.subject.sigma ~ dnorm(0.0, 1/100.0)T(0,)

    b.text.tau <- 1/(b.text.sigma^2)
    b.subject.tau <- 1/(b.subject.sigma^2)

    beta.text.sigma ~ dnorm(0.0, 1/100.0)T(0,)

    beta.text.tau <- 1/(beta.text.sigma^2)

    alpha.0 ~ dnorm(0, 1.0e-5)
    beta ~ dnorm(0, 1.0e-5)
    gamma ~ dnorm(0, 1.0e-5)
    b ~ dnorm(0, 1.0e-5)

}
