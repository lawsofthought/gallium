{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate predicted probabilities based on co-occurrence probabilities\n",
    "\n",
    "We aim to calculate \n",
    "$$\n",
    "\\mathrm{P}(w_k \\vert w_l) \\triangleq \\frac{ \\mathrm{P}(w_k,w_l) }{\\mathrm{P}(w_l)}, \n",
    "$$\n",
    "which is the empirical probability of observing word $w_k$ in some linguistic context, e.g., a short text, given that we've observed $w_l$ there.\n",
    "\n",
    "What is the probability of finding word $w_k$ and word $w_l$ in the same context, e.g. the same text? If we were to choose a text at random, and then choose a pair of words at random from that text, what is the probability of choosing the pair $w_k$ and $w_l$? If call the randomly chosen text text $j$, the probability of choosing the pair $w_k$ and $w_l$ at random is \n",
    "$$\n",
    "\\mathrm{P}(w_k,w_l\\vert \\textrm{text}=j) = \\frac{2 n_{jk} n_{jl}}{n_j (n_j-1)} \n",
    "$$\n",
    "where $n_{jk}$ is the number of occurrences of word $w_k$ in text $j$, $n_{jl}$ is the number of occurrences of word $w_l$ in text $j$, and $n_j$ is the total number of words in text $j$. If $w_k = w_l$ then the numerator above is $n_{jk} (n_{jk}-1)$. The total number of pairs in the text is $n_j (n_j-1)$ and of these, $2 n_{jk} n_{jl}$ are the words $w_k$ and $w_l$.  The total number of pairs in the corpus of $J$ document is\n",
    "$$\n",
    "\\sum_{j=1}^J n_j (n_j-1)\n",
    "$$\n",
    "and the total number of pairs in the corpus that are $w_k$ and $w_l$ is\n",
    "$$\n",
    "\\sum_{j=1}^j 2 n_{jk} n_{jl} \n",
    "$$\n",
    "and so\n",
    "$$\n",
    "\\mathrm{P}(w_k,w_l) = \\frac{\\sum_{j=1}^J n_j (n_j-1) }{ \\sum_{j=1}^j 2 n_{jk} n_{jl}} \n",
    "$$\n",
    "\n",
    "The total number of word pairs in text $j$ that contain $w_k$, which is $\\mathrm{P}(w_k)$, is\n",
    "$$\n",
    "\\mathrm{P}(w_k) = \\sum_{l\\neq k} 2 n_{jk} n_{jl} + n_{jk} (n_{jk}-1) = 2 n_{jk} (n-n_{jk}) + n_{jk} (n_{jk}-1)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import configobj\n",
    "import pandas\n",
    "import numpy\n",
    "import cPickle as pickle\n",
    "from utils import utils\n",
    "from utils import datautils\n",
    "from utils.datautils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_root = 'http://www.lawsofthought.org/shared'\n",
    "\n",
    "cache_directory = '_cache'\n",
    "\n",
    "filenames = {\n",
    "    'experiment_cfg' : [('Brismo.cfg',\n",
    "                         '909d9f8de483c4547f26fb4c34b91e12908ab5c144e065dc0fe6c1504b1f22c9')],\n",
    "    'text-corpus' : [('bnc_texts_78723408_250_500.txt.bz2', \n",
    "                      'dd8806f51088f7c8ad6c1c9bfadb6680c44bc5fd411e52970ea9c63596c83d34')],\n",
    "    'vocabulary' : [('bnc_vocab_49328.txt',\n",
    "                     '55737507ea9a2c18d26b81c0a446c074c6b8c72dedfa782c763161593e6e3b97')]\n",
    "}\n",
    "\n",
    "utils.curl(url_root, \n",
    "                 filenames['experiment_cfg'] + filenames['text-corpus'] + filenames['vocabulary'], \n",
    "                 cache=cache_directory,\n",
    "                 verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "memoranda = configobj.ConfigObj('_cache/Brismo.cfg')['text_memoranda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = open('_cache/bnc_vocab_49328.txt').read().split()\n",
    "vocab = datautils.Vocab(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Df = {}\n",
    "Df['recall'] = pandas.read_pickle('_cache/brisbane_06b643a_recall_results.pkl')\n",
    "\n",
    "recalled_words = sorted(set(Df['recall']['word'].values).intersection(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_words(text):\n",
    "    return [word for word in utils.tokenize(text) \n",
    "            if word in vocab.word2index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words = recalled_words[:]\n",
    "for text_name in memoranda:\n",
    "\n",
    "    inwords = memoranda[text_name]['inwords'].split(',')\n",
    "    outwords = memoranda[text_name]['outwords'].split(',')\n",
    "    text_words = text_to_words(memoranda[text_name]['text'])\n",
    "    \n",
    "    all_words.extend(inwords + outwords + text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cooccurrences = datautils.Cooccurrences('bnc_texts_78723408_250_500.txt.bz2', \n",
    "                                        cache='_cache', \n",
    "                                        vocabulary_list=vocabulary, \n",
    "                                        target_words=all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The following parallel processing is *very* memory hungry. It requires around 6GB per processor. I am limited to 64GB, so I only use 8 cores to avoid any memory overflow. The whole thing takes around 3hrs to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "\n",
    "if not use_cache:\n",
    "\n",
    "    def get_conditional_probabilities(text_name):\n",
    "\n",
    "        cooccurrences.init()\n",
    "\n",
    "        conditional_probabilities = {}\n",
    "\n",
    "        inwords = memoranda[text_name]['inwords'].split(',')\n",
    "        outwords = memoranda[text_name]['outwords'].split(',')\n",
    "        for prime_word in text_to_words(memoranda[text_name]['text']):\n",
    "            for target_word in inwords+outwords+recalled_words:\n",
    "                conditional_probabilities[(target_word, prime_word)]\\\n",
    "                = cooccurrences.conditional_probability(target_word, prime_word)\n",
    "\n",
    "        cooccurrences.deinit()\n",
    "\n",
    "        return conditional_probabilities\n",
    "\n",
    "\n",
    "    from ipyparallel import Client\n",
    "\n",
    "    clients = Client()\n",
    "\n",
    "    clients.block = True\n",
    "\n",
    "    clients[:].push(dict(memoranda = memoranda, \n",
    "                         recalled_words=recalled_words, \n",
    "                         text_to_words=text_to_words,\n",
    "                         tokenize=tokenize,\n",
    "                         vocab=vocab,\n",
    "                         cooccurrences=cooccurrences));\n",
    "\n",
    "    view = clients.load_balanced_view()\n",
    "\n",
    "    _conditional_probabilities = view.map(get_conditional_probabilities, memoranda.keys())\n",
    "\n",
    "    conditional_probabilities = {}\n",
    "    for _conditional_probabilities_i in _conditional_probabilities:\n",
    "        conditional_probabilities.update(_conditional_probabilities_i)\n",
    "\n",
    "    with open('_cache/conditional_probabilities.pkl', 'wb') as f:\n",
    "        pickle.dump(conditional_probabilities, f, protocol=2)\n",
    "\n",
    "else:\n",
    "    \n",
    "    with open('_cache/conditional_probabilities.pkl', 'rb') as f:\n",
    "        conditional_probabilities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = {}\n",
    "for text_name in memoranda:\n",
    "\n",
    "    inwords = memoranda[text_name]['inwords'].split(',')\n",
    "    outwords = memoranda[text_name]['outwords'].split(',')\n",
    "    \n",
    "    prime_words = text_to_words(memoranda[text_name]['text'])\n",
    "    \n",
    "    p[text_name] = {}\n",
    "    for target_word in inwords+outwords+recalled_words:\n",
    "        p[text_name][target_word] = 0.0\n",
    "        for prime_word in prime_words:\n",
    "            p[text_name][target_word] += conditional_probabilities[(target_word, prime_word)]\n",
    "        p[text_name][target_word] /= len(prime_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cooccurrence_predictions = {}\n",
    "for text_name in memoranda:\n",
    "    \n",
    "    _, n = text_name.split('_')\n",
    "    n = int(n)+1\n",
    "    \n",
    "    inwords = memoranda[text_name]['inwords'].split(',')\n",
    "    outwords = memoranda[text_name]['outwords'].split(',')\n",
    "    \n",
    "    for word in inwords+outwords+recalled_words:\n",
    "\n",
    "        prob = p[text_name][word]\n",
    "        cooccurrence_predictions[str(n) + '-' + word] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('_cache/cooccurrence_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(cooccurrence_predictions, f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictive_probabilities = []\n",
    "text_names = sorted(p.keys(), key=lambda arg: int(arg.split('_')[1]))\n",
    "for text_name in text_names:\n",
    "    f = []\n",
    "    for word in recalled_words:\n",
    "        f.append(p[text_name][word])\n",
    "    predictive_probabilities.append(f)\n",
    "\n",
    "predictive_probabilities = numpy.array(predictive_probabilities)\n",
    "\n",
    "predictive_probabilities = numpy.c_[predictive_probabilities, 1-predictive_probabilities.sum(1)]\n",
    "\n",
    "header = ','.join(recalled_words + ['ALTERNATIVE_WORD'])\n",
    "\n",
    "M = [header]\n",
    "for i,f in enumerate(predictive_probabilities):\n",
    "    M.append(text_names[i] + ',' + ','.join(map(str, f)))\n",
    "M = '\\n'.join(M)\n",
    "\n",
    "with open('_cache/cooccurrences_predictions_of_recalled_words.csv', 'w') as f:\n",
    "    f.write(M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
