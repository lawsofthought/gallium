{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate posterior predictions\n",
    "$$\n",
    "\\newcommand{\\given}{\\vert}\n",
    "\\newcommand{\\text}{\\mathrm{text}}\n",
    "\\newcommand{\\xtext}{w_1, w_2 \\ldots w_n}\n",
    "$$    \n",
    "\n",
    "The following calculates the posterior prediction over words conditioned a text.\n",
    "\n",
    "Given a text $\\text$, the posterior predictive distribution is, informally speaking, the distribution over words that are consistent with the discourse topics of the $\\text$. It is calculated as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{P}(w \\given \\phi, \\text, a, m) &= \\int \\mathrm{P}(w \\given \\phi, \\pi) \\mathrm{P}(\\pi \\given \\text, a, m) d\\pi,\\\\\n",
    "&= \\int \\big[ \\sum_{\\{x\\}} \\mathrm{P}(w \\given \\phi, x)\\mathrm{P}(x \\given \\pi) \\big] \\mathrm{P}(\\pi \\given \\text, a, m) d\\pi\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\mathrm{P}(\\pi \\given \\text, a, m)$ is the posterior distribution over topic distributions of text $\\text$ and $\\phi$ is the set of $K$ component topics and $a$, $m$ are the hyper-parameters of the Dirichlet prior over the per document mixing distribution.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "import utils\n",
    "import numpy\n",
    "\n",
    "from utils import topicmodels, utils\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "import datetime\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download files for the topic model\n",
    "\n",
    "Get data and some MCMC state samples for a HDPMM topic model. For each file, we provide its sha256 hash to check its integrity. If the files are already downloaded, the `curl` will not try to redownload them, but will just check their integrity. If the downloaded files are bz2 compressed, they will be uncompressed unless uncompressed versions exists already in the `cache_directory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_root = 'http://www.lawsofthought.org/shared'\n",
    "\n",
    "cache_directory = '_cache'\n",
    "\n",
    "filenames = {\n",
    "    'experiment_cfg' : [('Brismo.cfg',\n",
    "                         '909d9f8de483c4547f26fb4c34b91e12908ab5c144e065dc0fe6c1504b1f22c9')],\n",
    "    'corpus' : [('bnc_78723408_250_500_49328.npz.bz2', \n",
    "                 'b9d828f7697871e01a263b8f3978911c70ff45cab9af4c86fbb43c3baef969d9')],\n",
    "    'mcmc_samples' : [('hdptm_061216085831_7090_state_12946.npz.bz2', \n",
    "                       '9ba9850ff51fd60b679fd2af85cbaa4b3d69a2f31f4a0705475c0fffe3374330')]\n",
    "}\n",
    "\n",
    "utils.curl(url_root, \n",
    "                 filenames['experiment_cfg'] + filenames['corpus'] + filenames['mcmc_samples'], \n",
    "                 cache=cache_directory,\n",
    "                 verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load up the corpus and one of the state samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_data = utils.loadnpz(filenames['corpus'][0][0], \n",
    "                               cache=cache_directory,\n",
    "                               verbose=False)\n",
    "\n",
    "state = utils.loadnpz(filenames['mcmc_samples'][0][0],\n",
    "                         cache=cache_directory,\n",
    "                         verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = topicmodels.get_experiment_texts('Brismo.cfg', cache=cache_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = topicmodels.PosteriorPredictive(corpus_data, state, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cached_result = True\n",
    "cached_result = 'posterior_predictions.2017.01.20.1484887827.pkl'\n",
    "    \n",
    "with open('_cache/%s' % cached_result, 'rb') as f:\n",
    "    posterior_predictions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _posteriorprediction2str(text_name, vocab, K=10, f=lambda arg: arg):\n",
    "\n",
    "    words = utils.tokenize(texts[text_name])\n",
    "\n",
    "    p = {}\n",
    "    for k in numpy.flipud(posterior_predictions[text_name].argsort())[:K]:\n",
    "        p[k] = f(posterior_predictions[text_name][k])\n",
    "        \n",
    "    max_val = 1 + int(max(p.values()))\n",
    "        \n",
    "    results = []\n",
    "    for k, pk in p.items():\n",
    "        \n",
    "        if vocab[k] in words:\n",
    "            s = r'{\\fontsize{%2.2f}{%d}\\selectfont %s}' % (pk, max_val, vocab[k])\n",
    "        else:\n",
    "            s = r'{\\fontsize{%2.2f}{%d}\\selectfont \\textit{%s}}' % (pk, max_val, vocab[k])\n",
    "            \n",
    "        results.append(s)\n",
    "          \n",
    "    shuffle(results)\n",
    "    \n",
    "    doc = r'''\n",
    "    \\begin{figure}\n",
    "    \\begin{center}\n",
    "    \\fbox{\\begin{minipage}[t]{0.45\\textwidth}\n",
    "    {\\footnotesize \n",
    "    %s\n",
    "    }\n",
    "    \\centerline{\\adfast{3}\\adfast{3}\\adfast{3}\\adfast{3}\\adfast{3}\\adfast{3}\\adfast{3}\\adfast{3}\\adfast{3}\\adfast{3}}\n",
    "    \\begin{center}\n",
    "    %s\n",
    "    \\end{center}\n",
    "    \\end{minipage}\n",
    "    }\n",
    "    \\end{center}\n",
    "    \\end{figure}\n",
    "    ''' % (texts[text_name], '\\n'.join(results))\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def posteriorprediction2str(text_name):\n",
    "    \n",
    "    results = _posteriorprediction2str(text_name, \n",
    "                                       corpus_data['vocabulary'], \n",
    "                                       K=50, \n",
    "                                       f = lambda arg: 8*numpy.log(1000*arg))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for text_name in texts:\n",
    "    with open('%s.tex' % text_name, 'w') as f:\n",
    "        f.write(posteriorprediction2str(text_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mv text*tex /home/andrews/gitdev/papers/aubin/TeX/include/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
