{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Calculate predictions of words related to a text using word association norms\n",
    "\n",
    "Word association norms can, at least in most cases, be defined by a matrix $A$, such that \n",
    "$$\n",
    "A_{ij} \\triangleq \\text{frequency that word $w_i$ is stated as associated with word $w_j$}.\n",
    "$$\n",
    "\n",
    "Therefore, the conditional probability of word $w_i$ given $w_j$ is \n",
    "$$\n",
    "\\mathrm{P}(w_i \\vert w_j) = \\frac{A_{ij}}{\\sum_{i=1}^V A_{ij}},\n",
    "$$\n",
    "where $V$ is the total number of words in our vocabulary of response words.\n",
    "\n",
    "Given a text \n",
    "$$ \n",
    "\\textrm{text}_{j^\\prime} \\triangleq w_{j^\\prime 1}, w_{j^\\prime 2} \\ldots w_{j^\\prime n_{j^\\prime}}, \n",
    "$$\n",
    "the predicted probability that word $w_k$ is associated with $\\textrm{text}_{j^\\prime}$ is \n",
    "$$\n",
    "\\mathrm{P}(w_k \\vert \\textrm{text}_{j^\\prime}) = \\frac{1}{n_{j^\\prime}} \\sum_{i = 1}^{n_{j^\\prime}} \\mathrm{P}(w_k \\vert w_{j^\\prime i}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import configobj\n",
    "import numpy\n",
    "import pandas\n",
    "import cPickle as pickle\n",
    "\n",
    "from utils import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def text_to_words(text):\n",
    "    return [word for word in utils.tokenize(text) if word in word2index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "url_root = 'http://www.lawsofthought.org/shared'\n",
    "\n",
    "cache_directory = '../_cache'\n",
    "cache_fullpath = lambda path: os.path.join(cache_directory, path)\n",
    "\n",
    "filenames = {\n",
    "    'experiment_cfg' : [('Brismo.cfg',\n",
    "                         '909d9f8de483c4547f26fb4c34b91e12908ab5c144e065dc0fe6c1504b1f22c9')],\n",
    "    'corpus' : [('bnc_78723408_250_500_49328.npz.bz2', \n",
    "                 'b9d828f7697871e01a263b8f3978911c70ff45cab9af4c86fbb43c3baef969d9')]\n",
    "}\n",
    "\n",
    "utils.curl(url_root, \n",
    "                 filenames['experiment_cfg'] + filenames['corpus'], \n",
    "                 cache=cache_directory,\n",
    "                 verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stimuli = configobj.ConfigObj(cache_fullpath('Brismo.cfg'))['text_memoranda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corpus_data = utils.loadnpz('bnc_78723408_250_500_49328.npz.bz2',  \n",
    "                               cache='_cache',\n",
    "                               verbose=False)\n",
    "\n",
    "word2index = {w:i for i,w in enumerate(corpus_data['vocabulary'])}\n",
    "index2word = {i:w for i,w in enumerate(corpus_data['vocabulary'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texts = {}\n",
    "for key,value in stimuli.items():\n",
    "    texts[key] = text_to_words(value['text'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following assumes that the file `associations_en_05_01_2015.csv.bz2`, whose sha256 checksum is `06a527e5c9647f37a4a2ee0744a309f57f259e203238b87e0f466b74f7a6e63e` is available in the `_cache` directory. This is compressed csv file of word association norms collected at https://www.smallworldofwords.org/en and generously shared by Simon De Deyne (https://simondedeyne.me/). I am not at liberty to share this data presently, and so please contact Simon De Deyne, or Gert Storms in order to obtain it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_associations_data = utils.loadcsv('associations_en_05_01_2015.csv.bz2', cache=cache_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class WordAssociations(object):\n",
    "    \n",
    "\n",
    "    def __init__(self, word_associations_data):\n",
    "        \n",
    "        self.word_associations_data = word_associations_data\n",
    "            \n",
    "        self.build_associations()\n",
    "\n",
    "    def build_associations(self):\n",
    "        \n",
    "        self.associations = defaultdict(lambda: defaultdict(lambda : 1e-4))\n",
    "        \n",
    "        for row in self.word_associations_data:\n",
    "            \n",
    "            subject, stimulus, assoc1, assoc2, assoc3 = row.split(';')\n",
    "\n",
    "            for associate in (assoc1, assoc2, assoc3):\n",
    "            \n",
    "                self.associations[stimulus][associate] += 1\n",
    "                \n",
    "        self._normalize_associations()\n",
    "                \n",
    "    def _normalize_associations(self):\n",
    "        \n",
    "        for stimulus in self.associations:\n",
    "            values = numpy.array(self.associations[stimulus].values())\n",
    "            z = values.sum()\n",
    "            for associate in self.associations[stimulus]:\n",
    "                self.associations[stimulus][associate] /= z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_association = WordAssociations(word_associations_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_associations(text_name):\n",
    "    \n",
    "    '''\n",
    "    This implements\n",
    "     \n",
    "     \\mathrm{P}(w_k \\vert \\textrm{text}_{j^\\prime}) = \n",
    "     \\frac{1}{n_{j^\\prime}} \\sum_{i = 1}^{n_{j^\\prime}} \\mathrm{P}(w_k \\vert w_{j^\\prime i})\n",
    "     \n",
    "     (see above)\n",
    "     \n",
    "     where `associate` below corresponds to `w_k` \n",
    "     and so we calculate the probability of `associate` as a response for each word in the text.\n",
    "     And then average over all of these words.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    d = defaultdict(lambda : 0.0)\n",
    "\n",
    "    n = 0.0 \n",
    "    for word in texts[text_name]:\n",
    "        if word in word_association.associations:\n",
    "            n += 1\n",
    "            for associate, strength in word_association.associations[word].items():\n",
    "                d[associate] += strength\n",
    "    \n",
    "    for associate in d:\n",
    "        d[associate] /= n\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Df = {}\n",
    "Df['recall'] = pandas.read_pickle(cache_fullpath('brisbane_06b643a_recall_results.pkl'))\n",
    "\n",
    "recalled_words = sorted(set(Df['recall']['word'].values).intersection(corpus_data['vocabulary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stimuli_words = []\n",
    "for text_name in stimuli:\n",
    "    d = get_associations(text_name)\n",
    "    _, n = text_name.split('_')\n",
    "    n = int(n)+1\n",
    "    inwords = stimuli[text_name]['inwords'].split(',')\n",
    "    outwords = stimuli[text_name]['outwords'].split(',')\n",
    "    for word in inwords+outwords+recalled_words:\n",
    "        try:\n",
    "            p = d[word]\n",
    "            stimuli_words.append((str(n) + '-' + word, p))\n",
    "        except KeyError:\n",
    "            print('Unknown word in text %s: \"%s\"' % (text_name,word))\n",
    "\n",
    "associations_predictions = dict(stimuli_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(cache_fullpath('raw_associations_predictions.pkl'), 'wb') as f:\n",
    "    pickle.dump(associations_predictions, f, protocol=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
