{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import configobj\n",
    "import pandas\n",
    "import cPickle as pickle\n",
    "from utils import utils\n",
    "from utils import datautils\n",
    "from utils.datautils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_root = 'http://www.lawsofthought.org/shared'\n",
    "\n",
    "cache_directory = '_cache'\n",
    "\n",
    "filenames = {\n",
    "    'experiment_cfg' : [('Brismo.cfg',\n",
    "                         '909d9f8de483c4547f26fb4c34b91e12908ab5c144e065dc0fe6c1504b1f22c9')],\n",
    "    'text-corpus' : [('bnc_texts_78723408_250_500.txt.bz2', \n",
    "                      'dd8806f51088f7c8ad6c1c9bfadb6680c44bc5fd411e52970ea9c63596c83d34')],\n",
    "    'vocabulary' : [('bnc_vocab_49328.txt',\n",
    "                     '55737507ea9a2c18d26b81c0a446c074c6b8c72dedfa782c763161593e6e3b97')]\n",
    "}\n",
    "\n",
    "utils.curl(url_root, \n",
    "                 filenames['experiment_cfg'] + filenames['text-corpus'] + filenames['vocabulary'], \n",
    "                 cache=cache_directory,\n",
    "                 verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "memoranda = configobj.ConfigObj('_cache/Brismo.cfg')['text_memoranda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = open('_cache/bnc_vocab_49328.txt').read().split()\n",
    "vocab = datautils.Vocab(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Df = {}\n",
    "Df['recall'] = pandas.read_pickle('_cache/brisbane_06b643a_recall_results.pkl')\n",
    "\n",
    "recalled_words = sorted(set(Df['recall']['word'].values).intersection(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_words(text):\n",
    "    return [word for word in utils.tokenize(text) \n",
    "            if word in vocab.word2index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words = recalled_words[:]\n",
    "for text_name in memoranda:\n",
    "\n",
    "    inwords = memoranda[text_name]['inwords'].split(',')\n",
    "    outwords = memoranda[text_name]['outwords'].split(',')\n",
    "    text_words = text_to_words(memoranda[text_name]['text'])\n",
    "    \n",
    "    all_words.extend(inwords + outwords + text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cooccurrences = datautils.Cooccurrences('bnc_texts_78723408_250_500.txt.bz2', \n",
    "                                        cache='_cache', \n",
    "                                        vocabulary_list=vocabulary, \n",
    "                                        target_words=all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('_cache/conditional_probabilities.pkl', 'rb') as f:\n",
    "    conditional_probabilities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = {}\n",
    "for text_name in memoranda:\n",
    "\n",
    "    inwords = memoranda[text_name]['inwords'].split(',')\n",
    "    outwords = memoranda[text_name]['outwords'].split(',')\n",
    "    \n",
    "    prime_words = text_to_words(memoranda[text_name]['text'])\n",
    "    \n",
    "    p[text_name] = {}\n",
    "    for target_word in inwords+outwords+recalled_words:\n",
    "        p[text_name][target_word] = 0.0\n",
    "        for prime_word in prime_words:\n",
    "            p[text_name][target_word] += conditional_probabilities[(target_word, prime_word)]\n",
    "        p[text_name][target_word] /= len(prime_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F = []\n",
    "text_names = sorted(p.keys(), key=lambda arg: int(arg.split('_')[1]))\n",
    "for text_name in text_names:\n",
    "    f = []\n",
    "    for word in recalled_words:\n",
    "        f.append(p[text_name][word])\n",
    "    F.append(f)\n",
    "\n",
    "F = numpy.array(F)\n",
    "\n",
    "F = numpy.c_[F, 1-F.sum(1)]\n",
    "\n",
    "header = ','.join(recalled_words + ['ALTERNATIVE_WORD'])\n",
    "\n",
    "M = [header]\n",
    "for i,f in enumerate(F):\n",
    "    M.append(text_names[i] + ',' + ','.join(map(str, f)))\n",
    "M = '\\n'.join(M)\n",
    "\n",
    "with open('_cache/cooccurrences_predictions_of_recalled_words.csv', 'w') as f:\n",
    "    f.write(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
