{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text $j$, if there are $n_j$ words, there are $\\frac{n_j \\cdot (n_j - 1)}{2}$ pairs of words in total (where the pairs need not be cosecutive words in the text). Of all these pairs, there are $n_{jk} \\cdot n_{jl}$ pairs consisting of words $w_k$ and $w_l$, and $\\frac{n_{jk} \\cdot (n_{jk} - 1)}{2}$ pair consisting of word $w_k$ repeated twice, where $n_{jk}$ and $n_{jl}$ are the number of times that word $w_k$ and $w_l$, respectively, occur in text $j$.\n",
    "\n",
    "Across all of the texts in the corpus, the total number of cooccurrences of the words $w_k$ and $w_l$ is \n",
    "$$\n",
    "C_{kl} = \\begin{cases}\n",
    "\\sum_{j=1}^J n_{jk} \\cdot n_{jl},\\quad\\text{if $k\\neq l$}\\\\\n",
    "\\tfrac{1}{2} \\sum_{j=1}^J n_{jk} \\cdot (n_{jl}-1),\\quad\\text{if $k = l$}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "\n",
    "import configobj\n",
    "import pandas\n",
    "import numpy\n",
    "import os\n",
    "import cPickle as pickle\n",
    "from scipy import sparse\n",
    "from scipy.special import digamma\n",
    "from utils import utils\n",
    "from utils import datautils\n",
    "from collections import defaultdict\n",
    "from utils.datautils import tokenize\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_root = 'http://www.lawsofthought.org/shared'\n",
    "\n",
    "cache_directory = '../_cache'\n",
    "cache_fullpath = lambda path: os.path.join(cache_directory, path)\n",
    "\n",
    "filenames = {\n",
    "    'experiment_cfg' : [('Brismo.cfg',\n",
    "                         '909d9f8de483c4547f26fb4c34b91e12908ab5c144e065dc0fe6c1504b1f22c9')],\n",
    "    'text-corpus' : [('bnc_texts_78723408_250_500.txt.bz2', \n",
    "                      'dd8806f51088f7c8ad6c1c9bfadb6680c44bc5fd411e52970ea9c63596c83d34')],\n",
    "    'vocabulary' : [('bnc_vocab_49328.txt',\n",
    "                     '55737507ea9a2c18d26b81c0a446c074c6b8c72dedfa782c763161593e6e3b97')]\n",
    "}\n",
    "\n",
    "utils.curl(url_root, \n",
    "                 filenames['experiment_cfg'] + filenames['text-corpus'] + filenames['vocabulary'], \n",
    "                 cache=cache_directory,\n",
    "                 verbose=False)\n",
    "\n",
    "memoranda = configobj.ConfigObj(cache_fullpath('Brismo.cfg'))['text_memoranda']\n",
    "\n",
    "vocabulary = open(cache_fullpath('bnc_vocab_49328.txt')).read().split()\n",
    "vocab = datautils.Vocab(vocabulary)\n",
    "\n",
    "Df = {}\n",
    "Df['recall'] = pandas.read_pickle(cache_fullpath('brisbane_06b643a_recall_results.pkl'))\n",
    "\n",
    "recalled_words = sorted(set(Df['recall']['word'].values).intersection(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_words(text):\n",
    "    return [word for word in utils.tokenize(text) \n",
    "            if word in vocab.word2index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_random_words(K=5000, seed=10101):\n",
    "    random = numpy.random.RandomState(seed)\n",
    "    return [vocab.vocab[i] for i in random.permutation(len(vocab.vocab))[:5000]]\n",
    "\n",
    "all_words = recalled_words[:] #+ get_random_words()\n",
    "\n",
    "for text_name in memoranda:\n",
    "\n",
    "    inwords = memoranda[text_name]['inwords'].split(',')\n",
    "    outwords = memoranda[text_name]['outwords'].split(',')\n",
    "    text_words = text_to_words(memoranda[text_name]['text'])\n",
    "    \n",
    "    all_words.extend(inwords + outwords + text_words)\n",
    "    \n",
    "all_words = sorted(set(all_words).intersection(vocab.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cooccurrences = datautils.Cooccurrences('bnc_texts_78723408_250_500.txt.bz2', \n",
    "                                        cache=cache_directory,\n",
    "                                        vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = normalize(cooccurrences.C, norm='l1', axis=1)\n",
    "\n",
    "assert numpy.allclose(P.sum(1), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_mean = P.mean(0).A.flatten()\n",
    "p_squared_mean = P.power(2).mean(0).A.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = (p_mean - p_squared_mean)/(p_squared_mean - p_mean**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2102.8007198751156"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.exp(numpy.log(a).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
