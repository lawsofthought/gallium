{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicted Word Associates of Texts, based on Word Cooccurrence Statistics\n",
    "\n",
    "Here, we calculate the predicted word associates of the texts that are used in our text memory experiment. These predictions can be used to test a word association account of text memory, where word associations are defined in terms of cooccurrence statistics in the language.\n",
    "\n",
    "## A probabilistic model of word cooccurrences \n",
    "\n",
    "$$\n",
    "\\newcommand{\\data}{\\mathcal{D}}\n",
    "\\newcommand{\\Prob}[1]{\\mathrm{P}( #1 )}\n",
    "\\newcommand{\\given}{\\vert}\n",
    "$$\n",
    "\n",
    "If $\\mathcal{D}$ is a corpus of $J$ texts, i.e., $\\mathcal{D} = \\textrm{text}_1, \\textrm{text}_2 \\ldots \\textrm{text}_j$, the probability that word $w_k$ and $w_l$ both appear in text $j$ is \n",
    "$$\n",
    "\\Prob{w_k, w_l \\given \\textrm{text}_j} = \\int \\Prob{w_k, w_l \\given \\pi_j} \\Prob{\\pi_j \\given \\data} d\\pi_j\n",
    "$$\n",
    "and their average probability of co-occurrence in the corpus is \n",
    "$$\n",
    "\\Prob{w_k, w_l} = \\frac{1}{J} \\sum_{j=1}^J \\Prob{w_k, w_l \\given \\textrm{text}_j}.\n",
    "$$\n",
    "\n",
    "Here, $\\Prob{\\pi_j \\given \\data}$ is the posterior probability of $\\pi_j$, which is a categorical distribution over the length $V$ vocabulary. In other words, each text is modelled as a probability distribution over the entire vocabulary of words, and we infer this distribution from the corpus as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{P}(\\pi_j \\vert \\mathcal{D}) \n",
    "&= \\int \\mathrm{P}(\\pi_j \\vert \\mathcal{D}, a, m)\\mathrm{P}(a, m \\vert \\mathcal{D}),\\\\\n",
    "&\\approx \\mathrm{P}(\\pi_j \\vert \\mathcal{D}, \\hat{a}, \\hat{m})\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\hat{a}$ and $\\hat{m}$ are the posterior means of $a$ and $m$ (conditioned on $\\mathcal{D}$), respectively, and\n",
    "$$\n",
    "\\mathrm{P}(\\pi_j \\vert text_j) \\propto \\prod_{i=1}^{n_j} \\mathrm{P}(w_{ji} \\vert \\pi_j) \\mathrm{P}(\\pi_j \\vert a, m).\n",
    "$$\n",
    "If $\\mathrm{P}(\\pi_j \\vert a, m)$ is a Dirichlet distribution with location parameter $m$ and concentration parameter $a$, then the posterior distribution is \n",
    "$$\n",
    "\\mathrm{P}(\\pi_j \\vert text_j) = \\textrm{Dirichlet}(R_{j1} + a m_{1}, R_{j2} + a m_{2} \\ldots R_{jV} + a m_{V})\n",
    "$$\n",
    "From this, we have\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Prob{w_k, w_l \\given \\textrm{text}_j} &= \\int \\Prob{w_k, w_l \\given \\pi_j} \\Prob{\\pi_j \\given \\data, \\hat{a}, \\hat{m}} d\\pi_j,\\\\\n",
    "&=\\frac{\\Gamma(R_{j\\cdot} + a)}{\\prod_{v=1}^V \\Gamma(R_{jv} + am_v)}\n",
    "\\times\n",
    "\\frac{\\prod_{v=1}^V \\Gamma(R_{jv} + a m_{v} + \\mathbb{I}(w_k = v) + \\mathbb{I}(w_l = v) )}{\\Gamma(R_{j\\cdot} + a + 2) },\\\\\n",
    "&=\\frac{\\Gamma(R_{j\\cdot} + a)}{\\Gamma(R_{j\\cdot} + a + 2) }\n",
    "\\times \\prod_{v=1}^V \\frac{\\Gamma(R_{jv} + a m_{v}+ \\mathbb{I}(w_k = v) + \\mathbb{I}(w_l = v)) }{\\Gamma(R_{jv} + am_v)}\n",
    ",\\\\\n",
    "&=\\frac{(R_{jk} + a m_{k}) (R_{jl} + a m_{l}+ \\mathbb{I}(k = l))}{(R_{j\\cdot} + a + 1) (R_{j\\cdot} + a))}\n",
    "\\end{align}\n",
    "$$\n",
    "and so\n",
    "$$\n",
    "\\Prob{w_k, w_l} = \\frac{1}{J} \\sum_{j=1}^J \\frac{(R_{jk} + a m_{k}) (R_{jl} + a m_{l}+ \\mathbb{I}(k = l))}{(R_{j\\cdot} + a + 1) (R_{j\\cdot} + a))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import cPickle as pickle\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third party imports\n",
    "import configobj\n",
    "import numpy\n",
    "from scipy.special import gamma, gammaln\n",
    "from scipy.sparse import coo_matrix\n",
    "from gustav import models, utils # Available at https://lawsofthought.github.io/gustavproject/\n",
    "\n",
    "# Local imports\n",
    "from utils import utils\n",
    "from utils import datautils\n",
    "from utils.datautils import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following to verify the algebraic derivations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_the_math(iterations=100):\n",
    "\n",
    "    '''\n",
    "    Test the algebraic derivations shown above. Added for those in doubt.\n",
    "    '''\n",
    "    \n",
    "    def long_way(Rjdot, a, Rj, m, k, l, V):\n",
    "\n",
    "        wk = numpy.zeros(V)\n",
    "        wl = numpy.zeros(V)\n",
    "        wk[k] = 1\n",
    "        wl[l] = 1\n",
    "\n",
    "        return numpy.exp(gammaln(Rjdot + a) - gammaln(Rjdot + a + 2) +\\\n",
    "            sum([gammaln(Rj[v] + a*m[v] + wk[v] + wl[v]) - gammaln(Rj[v] + a*m[v]) for v in xrange(V)]))\n",
    "\n",
    "    def quick_way(Rjdot, a, Rj, m, k, l, V):\n",
    "\n",
    "        I = 1*(k == l)\n",
    "\n",
    "        return ((Rj[k] + a*m[k]) * (Rj[l] + a*m[l] + I)) / ((Rjdot + a + 1) * (Rjdot + a))\n",
    "\n",
    "    for iteration in xrange(iterations):\n",
    "\n",
    "        Rjdot = numpy.random.randint(0, 20)\n",
    "        a = numpy.random.uniform(0.1, 10.0)\n",
    "        V = numpy.random.randint(1000, 10000)\n",
    "        Rj = numpy.random.randint(0, 10, size=V)\n",
    "        m = numpy.random.rand(V)\n",
    "\n",
    "        k = numpy.random.randint(0, V)\n",
    "        l = numpy.random.randint(0, V)\n",
    "\n",
    "        assert numpy.isclose(long_way(Rjdot, a, Rj, m, k, l, V),\n",
    "                             quick_way(Rjdot, a, Rj, m, k, l, V))\n",
    "    \n",
    "        assert numpy.isclose(long_way(Rjdot, a, Rj, m, k, k, V),\n",
    "                             quick_way(Rjdot, a, Rj, m, k, k, V))\n",
    "\n",
    "check_the_math() # 100 iterations should take around 10 secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the necessary data files, and process them if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_root = 'http://www.lawsofthought.org/shared'\n",
    "\n",
    "cache_directory = '../_cache'\n",
    "\n",
    "filenames = {\n",
    "    'experiment_cfg' : [('Brismo.cfg',\n",
    "                         '909d9f8de483c4547f26fb4c34b91e12908ab5c144e065dc0fe6c1504b1f22c9')],\n",
    "    'text-corpus' : [('bnc_texts_78723408_250_500.txt.bz2', \n",
    "                      'dd8806f51088f7c8ad6c1c9bfadb6680c44bc5fd411e52970ea9c63596c83d34')],\n",
    "    'vocabulary' : [('bnc_vocab_49328.txt',\n",
    "                     '55737507ea9a2c18d26b81c0a446c074c6b8c72dedfa782c763161593e6e3b97')]\n",
    "}\n",
    "\n",
    "utils.curl(url_root, \n",
    "           filenames['experiment_cfg'] + filenames['text-corpus'] + filenames['vocabulary'], \n",
    "           cache=cache_directory,\n",
    "           verbose=False)\n",
    "\n",
    "memoranda = configobj.ConfigObj(os.path.join(cache_directory, \n",
    "                                             filenames['experiment_cfg'][0][0]))['text_memoranda']\n",
    "\n",
    "text_filename = os.path.join(cache_directory, \n",
    "                             os.path.splitext(filenames['text-corpus'][0][0])[0])\n",
    "\n",
    "vocabulary_filename = os.path.join(cache_directory, \n",
    "                                   filenames['vocabulary'][0][0])\n",
    "\n",
    "vocabulary = open(vocabulary_filename).read().split()\n",
    "vocab = datautils.Vocab(vocabulary)\n",
    "\n",
    "text_to_words = lambda text: [word for word in tokenize(text) if word in vocab.word2index]\n",
    "\n",
    "texts_as_words = {}\n",
    "for text_name in memoranda:\n",
    "    texts_as_words[text_name] = text_to_words(memoranda[text_name]['text'])\n",
    "\n",
    "prime_words = sorted(set(itertools.chain(*texts_as_words.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a corpus for use in a Multinomial Dirichlet Compound model, unless it is already made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparse_count_matrix_filename = 'bnc_78723408_250_500_vocab49328_sparse_matrix_count'\n",
    "sparse_count_matrix_filename_path = os.path.join(cache_directory, sparse_count_matrix_filename + '.npz')\n",
    "\n",
    "if not os.path.exists(sparse_count_matrix_filename_path):\n",
    "\n",
    "    utils.SparseCountMatrix.new(text_filename = text_filename,\n",
    "                                vocabulary_filename = vocabulary_filename,\n",
    "                                save_filename = sparse_count_matrix_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet multinomial compound (DMC) language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "\n",
    "def multinomial_dirichlet_compound_burnin(args):\n",
    "\n",
    "    cache_directory = '_cache' # From the POV of ipcluster\n",
    "    \n",
    "    iterations, seed = args\n",
    "    \n",
    "    model = models.MultinomialDirichletCompoundModel()\n",
    "\n",
    "    model.load_data(os.path.join(cache_directory, sparse_count_matrix_filename + '.npz'))\n",
    "\n",
    "    model.initialize(seed)\n",
    "\n",
    "    model.update(iterations=iterations, sample_c=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def multinomial_dirichlet_compound_sample(args):\n",
    "    \n",
    "    model, iterations, thin = args \n",
    "    \n",
    "    samples = model.sample(number_of_samples=iterations, thin=thin)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def make_burnin_args(iterations, seed, nchains=3):\n",
    "    \n",
    "    random = numpy.random.RandomState(seed=seed)\n",
    "    seeds = random.randint(101, 100001, size=nchains)\n",
    "    \n",
    "    return zip([iterations]*nchains, seeds)\n",
    "    \n",
    "def make_sampler_args(models, iterations, thin, nchains=3):\n",
    "    return zip(models, [iterations]*nchains, [thin]*nchains)\n",
    "\n",
    "def gelman_diag(psi):\n",
    "    \n",
    "    '''The Gelman-Rubin convergence diagnostic.'''\n",
    "    \n",
    "    m, n = psi.shape\n",
    "    \n",
    "    B = n * numpy.var(psi.mean(1), ddof=1)\n",
    "    \n",
    "    W = psi.var(1, ddof=1).mean() \n",
    "\n",
    "    V = (n-1)/n * W + B/n    \n",
    "\n",
    "    return numpy.sqrt(V/W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC sampler\n",
    "\n",
    "Run 3 parallel chains. Burn in each chain for 100 iterations, sample *b*, *c* and *psi* from each chain 100 times, dropping every second sample. This may seem like a small number of samples. However, from previous experience, we know that convergence will be rapid and the posterior distribution of the variables is sharply peaked.\n",
    "\n",
    "#### Some parameters\n",
    "\n",
    "* The **master seed**, which will make seeds that make seeds that make seeds, and son on ensuring complete reproducibility\n",
    "* The number of chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MASTER_SEED = 27431\n",
    "NCHAINS = 3\n",
    "NCORES = 16\n",
    "N_BURN_IN = 100\n",
    "N_SAMPLE = 100\n",
    "SAMPLE_THIN = 2\n",
    "\n",
    "samples_save_filename_basename = 'multinomial_dirichlet_chains_seed_%d_burn_%d_sample_%d_thin_%d.pkl' % (MASTER_SEED,\n",
    "                                                                                                         N_BURN_IN,\n",
    "                                                                                                         N_SAMPLE,\n",
    "                                                                                                         SAMPLE_THIN)\n",
    "\n",
    "samples_save_filename = os.path.join(cache_directory, \n",
    "                                     samples_save_filename_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(samples_save_filename):\n",
    "\n",
    "    # Start a cluster on the command line with \"ipcluster start -n NCORES\" \n",
    "    # where NCORES is at least as large as NCHAINS\n",
    "    \n",
    "    # Each chain takes about 6 hours (give or take)\n",
    "\n",
    "    from ipyparallel import Client\n",
    "\n",
    "    clients = Client()\n",
    "\n",
    "    clients.block = True\n",
    "\n",
    "    clients[:].push(dict(sparse_count_matrix_filename = sparse_count_matrix_filename))\n",
    "\n",
    "    with clients[:].sync_imports():\n",
    "        from gustav import models\n",
    "\n",
    "    view = clients.load_balanced_view()\n",
    "\n",
    "    models = view.map(multinomial_dirichlet_compound_burnin, \n",
    "                      make_burnin_args(N_BURN_IN, seed=MASTER_SEED, nchains=NCHAINS))\n",
    "\n",
    "    samples = view.map(multinomial_dirichlet_compound_sample,\n",
    "                       make_sampler_args(models, N_SAMPLE, SAMPLE_THIN))\n",
    "\n",
    "    with open(samples_save_filename, 'wb') as f:\n",
    "        pickle.dump(samples, file=f, protocol=2)\n",
    "        \n",
    "else:\n",
    "    \n",
    "    with open(samples_save_filename, 'rb') as f:\n",
    "        samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence diagnostics\n",
    "\n",
    "We'll check if *b*, *c*, and each variable of the vector variable *psi* have converged using the Gelman Rubin diagnostic. For the *psi*, we will also check if the means of samples of *psi* across the three chains are more or less identical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gelman_diag(key='b'):\n",
    "    return gelman_diag(numpy.array([samples[k][key] for k in xrange(NCHAINS)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0141675325123143"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gelman_diag('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99785578883547243"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gelman_diag('c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.049718484858517"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.max([gelman_diag(numpy.array([[psi[i] for psi in samples[k]['psi']] for k in xrange(NCHAINS)]))\n",
    " for i in xrange(len(samples[0]['psi'][0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.99999807,  0.99999808],\n",
       "       [ 0.99999807,  1.        ,  0.99999809],\n",
       "       [ 0.99999808,  0.99999809,  1.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.corrcoef(numpy.array([numpy.array(samples[k]['psi']).mean(0) for k in xrange(NCHAINS)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate sample mean of *b* and *psi*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = numpy.array([samples[k]['b'] for k in (0, 1, 2)]).mean()\n",
    "psi = numpy.array([samples[k]['psi'] for k in (0, 1, 2)]).mean(axis=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = models.MultinomialDirichletCompoundModel()\n",
    "model.load_data(os.path.join(cache_directory, sparse_count_matrix_filename + '.npz'))\n",
    "\n",
    "R = coo_matrix((model.values, (model.rows, model.cols)), shape=(model.J, model.V)).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\Prob{w_l \\given w_k} &= \\frac{\\Prob{w_k, w_l}}{\\Prob{w_k}} = \\frac{\\Prob{w_k, w_l}}{\\sum_{\\{w_l\\}} \\Prob{w_k, w_l}},\\\\\n",
    "\\Prob{w_k, w_l} &= \\frac{1}{J} \\sum_{j=1}^J \\frac{(R_{jk} + a m_{k}) (R_{jl} + a m_{l}+ \\mathbb{I}(k = l))}{(R_{j\\cdot} + a + 1) (R_{j\\cdot} + a))}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Pr_wl_given_wk(R, word_indices, b, psi):\n",
    "    \n",
    "    '''\n",
    "    Return predicted probability of all words conditioned on word k.\n",
    "    In other words, what is the predicted probability of observing any\n",
    "    given word given that we have observed word k.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    J, V = R.shape\n",
    "    \n",
    "    k_len = len(word_indices)\n",
    "    \n",
    "    I = numpy.zeros((k_len, V))\n",
    "        \n",
    "    for k in xrange(k_len):\n",
    "        I[k, word_indices[k]] = 1\n",
    "\n",
    "    p = numpy.zeros((k_len, V))\n",
    "    \n",
    "    for j in xrange(J):\n",
    "    \n",
    "        Rjam = (R[j] + b*psi).A.flatten()\n",
    "        Rjdot = R[j].sum()\n",
    "\n",
    "        Z = (Rjdot + b + 1) * (Rjdot + b)\n",
    "\n",
    "        for k in xrange(k_len):\n",
    "            p[k] += Rjam[word_indices[k]] * (Rjam + I[k]) / Z\n",
    "        \n",
    "    for k in xrange(k_len):\n",
    "        p[k] = p[k]/J\n",
    "        p[k] = p[k]/p[k].sum()\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#p = Pr_wl_given_wk(R, (10, 101, 10001), b, psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_prime_words = prime_words[:] # copy\n",
    "\n",
    "assignments = defaultdict(list)\n",
    "for k in itertools.cycle(xrange(NCORES-2)):\n",
    "    assignments[k].append(vocab.word2index[_prime_words.pop()])\n",
    "    \n",
    "    if not _prime_words:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = []\n",
    "\n",
    "for assignment in assignments.values():    \n",
    "    args.append((R, assignment, b, psi))\n",
    "\n",
    "def foo(args):\n",
    "    R, word_indices, b, psi = args\n",
    "    return word_indices, Pr_wl_given_wk(R, word_indices, b, psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing numpy on engine(s)\n"
     ]
    }
   ],
   "source": [
    "# This will take around 2 to 3 hrs on a 16 core system\n",
    "from ipyparallel import Client\n",
    "\n",
    "clients = Client()\n",
    "\n",
    "clients.block = True\n",
    "\n",
    "clients[:].push(dict(Pr_wl_given_wk = Pr_wl_given_wk))\n",
    "\n",
    "with clients[:].sync_imports():\n",
    "    import numpy\n",
    "\n",
    "view = clients.load_balanced_view()\n",
    "\n",
    "results = view.map(foo, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for result in results:\n",
    "    for i,v in enumerate(result[0]):\n",
    "        predictions[vocab.index2word[v]] = result[1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(predictions.keys()) == len(prime_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_topic(p, k=10):\n",
    "    return [vocab.index2word[w] for w in numpy.flipud(p.argsort())[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['war', 'time', 'people', 'world', 'government', 'life', 'day', 'found', 'left', 'home', 'set', 'house', 'local', 'system', 'national', 'public', 'party', 'head', 'country', 'social', 'called', 'children', 'told', 'form', 'hand', 'political', 'looked', 'days', 'power', 'family', 'support', 'major', 'women', 'held', 'change', 'times', 'company', 'night', 'money', 'control', 'information', 'business', 'means', 'school', 'eyes', 'cent', 'john', 'development', 'view', 'service', 'including', 'period', 'main', 'terms', 'mind', 'half', 'past', 'question', 'round', 'policy', 'level', 'result', 'economic', 'act', 'months', 'effect', 'real', 'position', 'week', 'feel', 'brought', 'office', 'sense', 'difficult', 'water', 'hard', 'future', 'labour', 'market', 'taking', 'words', 'provide', 'close', 'body', 'century', 'society', 'book', 'matter', 'idea', 'late', 'international', 'law', 'father', 'line', 'front', 'south', 'experience', 'white', 'process', 'west']\n"
     ]
    }
   ],
   "source": [
    "print(get_topic(predictions['war'], k=100))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
